{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpsVf7maGroB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import zipfile\n",
        "import shutil\n",
        "import os, shutil\n",
        "from google.colab import files\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_kaggle(): #connecting to kaggle\n",
        "        print(\"Please upload your kaggle.json file\")\n",
        "        uploaded = files.upload()\n",
        "        os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "        for fn in uploaded.keys():\n",
        "            shutil.move(fn, '/root/.kaggle/kaggle.json')\n",
        "        os.chmod('/root/.kaggle/kaggle.json', 600)\n",
        "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "        api = KaggleApi()\n",
        "        api.authenticate()\n",
        "        print(\"Kaggle API authenticated successfully\")\n",
        "        return api"
      ],
      "metadata": {
        "id": "-REzA7c4H8A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_kaggle_dataset(dataset_name, download_path='/content', unzip=True, force_download=False, api=None):\n",
        "    dataset_folder = dataset_name.split(\"/\")[-1]\n",
        "    dataset_path = os.path.join(download_path, dataset_folder)\n",
        "\n",
        "    if os.path.exists(dataset_path) and not force_download:\n",
        "        print(f\"Dataset already exists at {dataset_path}\")\n",
        "        return dataset_path\n",
        "\n",
        "    if api is None:\n",
        "        api = setup_kaggle()\n",
        "        if api is None:\n",
        "            print(\"Kaggle API setup failed.\")\n",
        "            return None\n",
        "    try:\n",
        "        print(f\"Downloading {dataset_name}...\")\n",
        "        api.dataset_download_files(dataset_name, path=download_path, unzip=unzip)\n",
        "        print(\"Download completed.\")\n",
        "        return dataset_path\n",
        "    except Exception as e:\n",
        "        print(f\"Download failed: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "lISo8qSjHeFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageFolderDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, verbose=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]) #sort labels\n",
        "\n",
        "        for label, class_name in enumerate(self.classes):\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                img_path = os.path.join(class_dir, img_name)\n",
        "                if os.path.isfile(img_path) and img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.labels.append(label)\n",
        "\n",
        "            if verbose: #מדפיס כמה תמונות נמצאות בתיקייה של כל מחלקה.\n",
        "                print(f\"  Class '{class_name}': {sum(1 for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg')))} images\")\n",
        "\n",
        "        if verbose: #מדפיס את הסיכום: כמה תמונות וקטגוריות נמצאו.\n",
        "            print(f\"Loaded {len(self.image_paths)} images from {len(self.classes)} classes\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths) #כמה תמונות יש בדטה סט\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            image = Image.new('RGB', (224, 224))  # Placeholder\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long) #מחזיר זוג: התמונה שעברה טרנספורמציה, והתווית כ־Tensor של PyTorch.\n"
      ],
      "metadata": {
        "id": "aitl15nrH8qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_rate=0.5):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = models.densenet121(weights='IMAGENET1K_V1')\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "ccsVegfyt-c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_precision_recall_f1(cm):\n",
        "    \"\"\"\n",
        "    Compute precision, recall and F1-score based on confusion matrix (only for binary classification).\n",
        "    \"\"\"\n",
        "\n",
        "    if cm.shape != (2, 2):\n",
        "        raise ValueError(\"This function supports only binary classification (2 classes).\")\n",
        "\n",
        "    TN, FP, FN, TP = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1 Score:  {f1_score:.4f}\")\n",
        "\n",
        "    return precision, recall, f1_score"
      ],
      "metadata": {
        "id": "fsPoZF4E2_Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device, class_names=None):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    TN=cm[0,0]\n",
        "    FP=cm[0,1]\n",
        "    FN=cm[1,0]\n",
        "    TP=cm[1,1]\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    classes = class_names if class_names is not None else np.arange(cm.shape[0])\n",
        "\n",
        "    # Set ticks and labels\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=classes,\n",
        "           yticklabels=classes,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Label each cell with its count and TP/FP/FN/TN\n",
        "    thresh = cm.max() / 2.\n",
        "    if cm.shape == (2, 2):  # Binary classification\n",
        "        labels = np.array([[\"TN\", \"FP\"],\n",
        "                           [\"FN\", \"TP\"]])\n",
        "        for i in range(2):\n",
        "            for j in range(2):\n",
        "                ax.text(j, i, f\"{labels[i, j]}\\n{cm[i, j]}\",\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    else:\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, str(cm[i, j]),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return TN, FP, FN, TP"
      ],
      "metadata": {
        "id": "1oYFqlDWkPtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_eval(model, train_loader, val_loader, criterion, optimizer, scheduler=None, epochs=10, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Training on {device}\")\n",
        "    model.to(device)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    #train\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
        "        for batch_X, batch_y in train_bar:#loop for each batch\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += batch_y.size(0)\n",
        "            correct_train += (predicted == batch_y).sum().item()\n",
        "\n",
        "            train_bar.set_postfix({'loss': loss.item(), 'acc': correct_train / total_train})\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "\n",
        "        # ---------- Evaluation after each epoch ----------\n",
        "        model.eval()\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = correct_val / total_val\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
        "        print(f\"  Val Loss:   {val_loss:.4f}, Val Acc:   {val_accuracy:.4f}\")\n",
        "\n",
        "    # ---------- Plot Results ----------\n",
        "    epochs_range = range(1, epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "ihe--XSFIkxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy1 = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
        "    print(f\"1.Test Accuracy: {accuracy1:.4f}\")\n",
        "    TN, FP, FN, TP=plot_confusion_matrix(model, test_loader, device)\n",
        "    accuracy2 = (TP + TN) / (TP + TN + FP + FN)\n",
        "    precision2 = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall2 = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "    print(f\"2 Test Accuracy CM: {accuracy2:.4f}\")\n",
        "    print(f\"Precision CM: {precision2:.4f}\")\n",
        "    print(f\"Recall CM: {recall2:.4f}\")\n",
        "\n",
        "    accuracy3 = np.mean(all_preds == all_labels)\n",
        "    precision3 = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall3 = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"3 Test Accuracy: {accuracy3:.4f}\")\n",
        "    print(f\"3 Precision (macro): {precision3:.4f}\")\n",
        "    print(f\"3 Recall (macro): {recall3:.4f}\")\n",
        "\n",
        "    return all_preds, all_labels"
      ],
      "metadata": {
        "id": "uS_MDSNfKxSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_paths(train_path, val_path,test_path):\n",
        "    \"\"\"\n",
        "    Checks if the provided paths for the training and validation datasets exist and are valid directories.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to the training dataset.\n",
        "        val_path (str): Path to the validation dataset.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if both paths exist and are valid directories, False otherwise.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(train_path):\n",
        "        print(f\"Training path '{train_path}' does not exist.\")\n",
        "        return False\n",
        "    if not os.path.isdir(train_path):\n",
        "        print(f\"Training path '{train_path}' is not a valid directory.\")\n",
        "        return False\n",
        "\n",
        "    if not os.path.exists(val_path):\n",
        "        print(f\"Validation path '{val_path}' does not exist.\")\n",
        "        return False\n",
        "    if not os.path.isdir(val_path):\n",
        "        print(f\"Validation path '{val_path}' is not a valid directory.\")\n",
        "        return False\n",
        "    if not os.path.exists(test_path):\n",
        "        print(f\"Validation path '{test_path}' does not exist.\")\n",
        "        return False\n",
        "    if not os.path.isdir(test_path):\n",
        "        print(f\"Validation path '{test_path}' is not a valid directory.\")\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "nCaXj1bfQys2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Installing required packages...\")\n",
        "    !pip install -q kaggle\n",
        "\n",
        "    # Set up Kaggle and download dataset\n",
        "    api = setup_kaggle()\n",
        "    dataset_path = download_kaggle_dataset(\"pkdarabi/diagnosis-of-diabetic-retinopathy\", api=api)\n",
        "    if dataset_path is None:\n",
        "        print(\"Dataset not found.\")\n",
        "        return\n",
        "    dataset_path = '/content/Diagnosis of Diabetic Retinopathy'\n",
        "    train_path = os.path.join(dataset_path, 'train')\n",
        "    val_path = os.path.join(dataset_path, 'valid')\n",
        "    test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "    if not check_paths(train_path, val_path, test_path):\n",
        "      print(\"Dataset not found.\")\n",
        "      return\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = ImageFolderDataset(train_path, transform=transform)\n",
        "    val_dataset = ImageFolderDataset(val_path, transform=transform)\n",
        "    test_dataset= ImageFolderDataset(test_path, transform=transform)\n",
        "\n",
        "    if (train_dataset.classes != val_dataset.classes) or (val_dataset.classes != test_dataset.classes):\n",
        "        print(\"Warning: Training validation and test class labels don't match.\")\n",
        "\n",
        "    num_classes = len(train_dataset.classes)\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    class_mapping = {i: cls for i, cls in enumerate(train_dataset.classes)}\n",
        "    with open('class_mapping.json', 'w') as f:\n",
        "        json.dump(class_mapping, f)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = Classifier(num_classes, dropout_rate=0.2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
        "\n",
        "    model, history = train_model_with_eval(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=100)\n",
        "    test_preds, test_labels=test_model(model, test_loader)\n",
        "\n",
        "    # Save model\n",
        "    model_path = 'final_model.pth'\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'num_classes': num_classes,\n",
        "        'class_mapping': class_mapping\n",
        "    }, model_path)\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        shutil.copy(model_path, '/content/drive/MyDrive/final_model.pth')\n",
        "        print(\"Model saved to Google Drive.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save model to Google Drive: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "QWBVJlZXL0eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLw3jf7fy0RJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}